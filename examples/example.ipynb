{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8l59XCJi08e4"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    '''\n",
        "    Input (y, A, H, P)\n",
        "    y, observation vector\n",
        "    P, Covariance\n",
        "\n",
        "    x(t+1) = A x(t) + ξ(t)\n",
        "    y(t) = H x(t) + ω(t)\n",
        "    x ∈ R^n, state\n",
        "    y ∈ R^m, observation\n",
        "    {ξ}_t∈Z, uncorrelated zero-mean process\n",
        "    {ω}_t∈Z, measurement noise vectors\n",
        "    E[ξ(t)ξ(t)^T] = Q\n",
        "    E[ω(t)ω(t)^T] = R\n",
        "    Q & R, unknown\n",
        "    P, error covariance matrix\n",
        "    ̂x(0) = m_0, P(t_0) = P_0\n",
        "\n",
        "    Prediction problem\n",
        "    ̂y_P(T) = H ̂x_P(T)\n",
        "    ̂x(t+1) = A ̂x(t) + L(t) ( y(t) - H x̂(t) )\n",
        "\n",
        "    Kalman gain\n",
        "    L(t) := A P(t) H^T ( H P(t) H^T + R )^(-1)\n",
        "    P(t) = E[(x(t) - ̂x(t))(x(t) - ̂x(t))^T]\n",
        "    P(t+1) = ( A - L(t) H ) P(t) A^T + Q\n",
        "    P converges when (A,H) is observable and the (A, Q^(1/2)) is controllable\n",
        "    L_∞ = A P_∞ H^T ( H P_∞ H^T + R )^(-1)\n",
        "    '''\n",
        "    def __init__(self, A, H, gpu=True):\n",
        "        super().__init__()\n",
        "        if gpu:\n",
        "            if torch.cuda.is_available():\n",
        "                gpu = 'cuda'\n",
        "            elif torch.backends.mps.is_available():\n",
        "                gpu = 'mps'\n",
        "            else:\n",
        "                gpu = 'cpu'\n",
        "        else:\n",
        "            gpu = 'cpu'\n",
        "        self.device = torch.device(gpu)\n",
        "        print('Using device:', self.device)\n",
        "        n = A.shape[0]\n",
        "        m = H.shape[0]\n",
        "        weights = torch.distributions.Uniform(0,0.5).sample((n,m))\n",
        "        self.A = torch.Tensor(A).to(self.device)\n",
        "        self.H = torch.Tensor(H).to(self.device)\n",
        "        self.weights = torch.nn.Parameter(weights)\n",
        "\n",
        "    def forward(self, y, N):\n",
        "        A = self.A\n",
        "        H = self.H\n",
        "        L = self.weights\n",
        "        y = torch.Tensor(y).to(self.device)\n",
        "        n = A.shape[0]\n",
        "        m = y.shape[0]\n",
        "        xhat = torch.zeros((n,N+1), device=self.device)\n",
        "        yhat = torch.zeros((m,N), device=self.device)\n",
        "        for k in range(N):\n",
        "            xhat[:,k+1] = A @ xhat[:,k] + L @ (y[:,k] - H @ xhat[:,k])\n",
        "            yhat[:,k] = H @ xhat[:,k]\n",
        "        return yhat\n",
        "\n",
        "def sgd(model, y, N, steps=1000, lr=1e-3, opt_params='adam', gpu=True):\n",
        "    '''\n",
        "    y, observation vector\n",
        "    N, time steps\n",
        "    steps, optimising num of loops\n",
        "    lr, learning rate\n",
        "    '''\n",
        "    #losses = []\n",
        "    #gains = []\n",
        "    losses = np.zeros(steps)\n",
        "    gains = np.zeros((model.H.shape[1],model.H.shape[0],steps))\n",
        "\n",
        "    if opt_params == 'adam':\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    elif opt_params == 'sgd':\n",
        "        opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    else:\n",
        "        assert True, \"Not supported optimizer\"\n",
        "\n",
        "    if gpu:\n",
        "        if torch.cuda.is_available():\n",
        "            gpu = 'cuda'\n",
        "        elif torch.backends.mps.is_available():\n",
        "            gpu = 'mps'\n",
        "        else:\n",
        "            gpu = 'cpu'\n",
        "    else:\n",
        "        gpu = 'cpu'\n",
        "    device = torch.device(gpu)\n",
        "    y = torch.Tensor(y).to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    pbar = tqdm(total=steps, unit='epochs')\n",
        "    for i in range(steps):\n",
        "        preds = model(y,N)\n",
        "        loss = torch.functional.F.mse_loss(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        #losses.append(float(loss))\n",
        "        #gains.append(model.weights.detach())\n",
        "        losses[i] = loss\n",
        "        gains[:,:,i] = model.weights.cpu().detach()\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "    preds = model(y,N).cpu().detach().numpy()\n",
        "    #losses = np.array(losses)\n",
        "    #gains = np.array(gains)\n",
        "    return losses, preds, gains\n",
        "\n",
        "def simple_mass_spring_damp(T, Q, R, P, Ts=0.1):\n",
        "    '''\n",
        "    m x''(t) + c x'(t) + k x(t) = 0\n",
        "    m, mass\n",
        "    c, damping coefficient\n",
        "    k, spring constant\n",
        "    F, force\n",
        "    Ts, time step\n",
        "    N, simulation length\n",
        "\n",
        "    E[ξ(t)ξ(t)^T] = Q\n",
        "    E[ω(t)ω(t)^T] = R\n",
        "\n",
        "    x(t+1) = A x(t) + ξ(t)\n",
        "    y(t) = H x(t) + ω(t)\n",
        "    {ξ}_t∈Z, uncorrelated zero-mean process\n",
        "    {ω}_t∈Z, measurement noise vectors\n",
        "    x(t) ∈ R^n, state of the system\n",
        "    y(t) ∈ R^m\n",
        "    '''\n",
        "    # (A,H) are known\n",
        "    H = np.array([[1.0, 0.0]])\n",
        "    C = 4\n",
        "    K = 2\n",
        "    M = 20\n",
        "    F = 1\n",
        "    N = int(T/Ts)\n",
        "    A = np.array([[1, Ts], [Ts*(-K/M), 1+Ts*(-C/M)]])\n",
        "    B = np.array([0, Ts*(1/M)])\n",
        "    u = F\n",
        "\n",
        "    n = A.shape[0]\n",
        "    m = H.shape[0]\n",
        "    x = np.zeros((n,N+1))\n",
        "    y = np.zeros((m,N))\n",
        "    m_0 = np.random.normal(0, P, size=n)\n",
        "    #ξ = np.random.multivariate_normal(np.zeros(n), Q, N+1).T * np.sqrt(Ts)\n",
        "    #ω = np.random.multivariate_normal(np.zeros(m), R, N).T * 1/np.sqrt(Ts)\n",
        "    ξ = np.random.multivariate_normal(np.zeros(n), Q, N+1).T\n",
        "    ω = np.random.multivariate_normal(np.zeros(m), R, N).T\n",
        "    x[:,0] = m_0\n",
        "    for k in range(N):\n",
        "        x[:,k+1] = A @ x[:,k] + B * u + ξ[:,k]\n",
        "        y[:,k] = H @ x[:,k] + ω[:,k]\n",
        "    return x[:,:-1], y, A, H"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n, m = 2, 1\n",
        "Q = np.eye(n) * 0.1\n",
        "R = np.eye(m) * 0.1\n",
        "P_0 = 0.05\n",
        "Ts = 0.1\n",
        "T = 100\n",
        "steps = 1000\n",
        "N = int(T/Ts)\n",
        "\n",
        "print('Horizon:', T)\n",
        "x, y, A, H = simple_mass_spring_damp(T, Q, R, P_0)\n",
        "t = np.arange(0, T, Ts)\n",
        "model = Model(A, H, gpu=True)\n",
        "losses, preds, weights = sgd(model, y, N, steps, lr=8e-3, gpu=True)\n",
        "print(\"Kalman Gain L: \\n\", weights[:,:,-1])\n",
        "\n",
        "# APA^T - APH^T (HPH^T + R)^(-1) HPA^T + Q - P = 0\n",
        "P = scipy.linalg.solve_discrete_are(A.T,H.T,Q,R)\n",
        "L = A @ P @ H.T @ np.linalg.inv(H @ P @ H.T + R)\n",
        "print(\"P: \\n\", P)\n",
        "print(\"L: \\n\", L)\n",
        "\n",
        "x, y, A, H = simple_mass_spring_damp(T, Q, R, P_0)\n",
        "preds = model(y,N).cpu().detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmcNBDWF8jZP",
        "outputId": "74852a38-9266-48e3-b0c8-9274e9a0e0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Horizon: 100\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 205/1000 [02:16<07:44,  1.71epochs/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(y.shape[0]):\n",
        "    plt.figure()\n",
        "    plt.plot(t, x[i])\n",
        "    plt.plot(t, preds[i], color='r')\n",
        "    plt.title('Simulation of Mass-Spring-Damper System')\n",
        "    plt.xlabel('t [s]')\n",
        "    plt.ylabel('x(t)')\n",
        "    plt.grid()\n",
        "    plt.legend([\"x\", \"filtered x\"])\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(t, y[i])\n",
        "    plt.plot(t, preds[i], color='r')\n",
        "    plt.title('Simulation of Mass-Spring-Damper System')\n",
        "    plt.xlabel('t [s]')\n",
        "    plt.ylabel('y(t)')\n",
        "    plt.grid()\n",
        "    plt.legend([\"y\", \"filtered y\"])\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(losses)\n",
        "    plt.title('Loss Function')\n",
        "    plt.xlabel('n [steps]')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print(weights)\n",
        "    print(weights.shape)\n",
        "    for j in range(n):\n",
        "        plt.figure()\n",
        "        plt.plot(weights[j][i])\n",
        "        plt.axhline(L[j], linestyle='--')\n",
        "        plt.title('Kalman Gain')\n",
        "        plt.xlabel('n [steps]')\n",
        "        plt.legend([\"Gain\", \"Optimal Gain\"])\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure()\n",
        "        plt.semilogy((L[j] - weights[j][i])**2)\n",
        "        plt.title('MSE Kalman Gain')\n",
        "        plt.xlabel('n [steps]')\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "CUj7XbTT82tH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}